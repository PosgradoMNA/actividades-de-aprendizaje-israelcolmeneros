{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation and Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation\n",
    "-Como funciona el modelo en el mundo real?\n",
    "-test & validation data\n",
    "-70% training, 30% test\n",
    "-crossvalidation, separar la data en K numero de sub conjuntos, se usa esas particiones para hacer el training/test de la data\n",
    "-cross_val_prediction() para que el output sea una prediccion y no una medicion de accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting, Underfitting and Model Selection\n",
    "-Underfiting/Overfiting\n",
    "-Queremos algo entre under y over, para que tenga el mejor performance con data real\n",
    "-Si vamos aumentando la complejidad de nuestro modelo podemos llegar a un overfiting, pero si es muy simple es probable que no tenga un buen accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression \n",
    "-Previene overfiting\n",
    "-introduce Alpha, donde ayuda a darle pesos a cada factor de la ecuacion\n",
    "-podemos determinar el mejor alpha, al hacer varias iteraciones hasta encontrar cual es la que da menos error\n",
    "-Entre mas alpha, menos overfiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search\n",
    "-Hyperparameters\n",
    "-Podemos usar la funcion de grid search para identificar los mejores parametros de un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAB/NOTEBOOK\n",
    "#Q1\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_data, y_data, test_size=0.4, random_state=0) \n",
    "print(\"number of test samples :\", x_test1.shape[0])\n",
    "print(\"number of training samples:\",x_train1.shape[0])\n",
    "#Q2\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_data, y_data, test_size=0.4, random_state=0)\n",
    "lre.fit(x_train1[['horsepower']],y_train1)\n",
    "lre.score(x_test1[['horsepower']],y_test1)\n",
    "#Q3\n",
    "Rc=cross_val_score(lre,x_data[['horsepower']], y_data,cv=2)\n",
    "Rc.mean()\n",
    "#Q4a\n",
    "pr1=PolynomialFeatures(degree=2)\n",
    "#Q4b\n",
    "x_train_pr1=pr1.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "\n",
    "x_test_pr1=pr1.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "#Q4c\n",
    "x_train_pr1.shape #there are now 15 features\n",
    "#Q4d\n",
    "poly1=LinearRegression().fit(x_train_pr1,y_train)\n",
    "#Q4e\n",
    "yhat_test1=poly1.predict(x_test_pr1)\n",
    "\n",
    "Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n",
    "\n",
    "DistributionPlot(y_test, yhat_test1, \"Actual Values (Test)\", \"Predicted Values (Test)\", Title)\n",
    "#Q4f\n",
    "#The predicted value is higher than actual value for cars where the price $10,000 range\n",
    "#Q5\n",
    "RigeModel = Ridge(alpha=10) \n",
    "RigeModel.fit(x_train_pr, y_train)\n",
    "RigeModel.score(x_test_pr, y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
